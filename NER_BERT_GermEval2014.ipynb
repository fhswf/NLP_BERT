{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_BERT_GermEval2014.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOK/L1iJjerONuLV+U3CQPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fhswf/NLP_BERT/blob/master/NER_BERT_GermEval2014.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBJdh8EAYgt",
        "colab_type": "text"
      },
      "source": [
        "#### License\n",
        "\n",
        "Copyright 2020 Christian Gawron (gawron.christian@fh-swf.de)\n",
        "\n",
        "Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "\n",
        "Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t0Ql-_G8Vrj",
        "colab_type": "text"
      },
      "source": [
        "# Fine-Tuning BERT for German Named Entity Recognition\n",
        "\n",
        "## Download and clean data for GermEval 2014 NER task\n",
        "\n",
        "The following lines download the data set and convert it to a format compatible with CoNLL 2003."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df8GwQgl73cx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d307ef35-26e6-42aa-ab2e-324beb5b29c1"
      },
      "source": [
        "!test -d data/ner || mkdir -p data/ner\n",
        "!test -e data/ner/train.txt.tmp || curl -L 'https://sites.google.com/site/germeval2014ner/data/NER-de-train.tsv?attredirects=0&d=1' | grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > data/ner/train.txt.tmp\n",
        "!test -e data/ner/dev.txt.tmp || curl -L 'https://sites.google.com/site/germeval2014ner/data/NER-de-dev.tsv?attredirects=0&d=1' | grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > data/ner/dev.txt.tmp\n",
        "!test -e data/ner/test.txt.tmp || curl -L 'https://sites.google.com/site/germeval2014ner/data/NER-de-test.tsv?attredirects=0&d=1' | grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > data/ner/test.txt.tmp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   566    0   566    0     0   2388      0 --:--:-- --:--:-- --:--:--  2388\n",
            "100 7697k    0 7697k    0     0  4536k      0 --:--:--  0:00:01 --:--:-- 6177k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   564    0   564    0     0   9096      0 --:--:-- --:--:-- --:--:--  9096\n",
            "100  706k    0  706k    0     0  2388k      0 --:--:-- --:--:-- --:--:-- 2388k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   565    0   565    0     0  12282      0 --:--:-- --:--:-- --:--:-- 12282\n",
            "100 1643k    0 1643k    0     0  4861k      0 --:--:-- --:--:-- --:--:-- 4861k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rpu4EGj_lFI",
        "colab_type": "text"
      },
      "source": [
        "### Data cleanup\n",
        "The GermEval 2014 data set contains some characters which cannot be parsed by BERT (see [README from original example](https://github.com/huggingface/transformers/blob/master/examples/token-classification/README.md)).\n",
        "\n",
        "The following code (see [GitHub](https://github.com/huggingface/transformers/blob/master/examples/token-classification/scripts/preprocess.py)) by Stefan Schweter filters these tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwgWjNlE_YQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL = 'bert-base-german-dbmdz-cased' #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dRxPdSz_Lsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "dataset = sys.argv[1]\n",
        "model_name_or_path = sys.argv[2]\n",
        "max_len = int(sys.argv[3])\n",
        "\n",
        "subword_len_counter = 0\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "max_len -= tokenizer.num_special_tokens_to_add()\n",
        "\n",
        "with open(dataset, \"rt\") as f_p:\n",
        "    for line in f_p:\n",
        "        line = line.rstrip()\n",
        "\n",
        "        if not line:\n",
        "            print(line)\n",
        "            subword_len_counter = 0\n",
        "            continue\n",
        "\n",
        "        token = line.split()[0]\n",
        "\n",
        "        current_subwords_len = len(tokenizer.tokenize(token))\n",
        "\n",
        "        # Token contains strange control characters like \\x96 or \\x95\n",
        "        # Just filter out the complete line\n",
        "        if current_subwords_len == 0:\n",
        "            continue\n",
        "\n",
        "        if (subword_len_counter + current_subwords_len) > max_len:\n",
        "            print(\"\")\n",
        "            print(line)\n",
        "            subword_len_counter = current_subwords_len\n",
        "            continue\n",
        "\n",
        "        subword_len_counter += current_subwords_len\n",
        "\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0GlhrSH8TYt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}